attach(survey)
View(survey)
str(survey)
names(survey)
nrow(survey)
ncol(survey)
fix(survey)
library(QuantPsyc)
library(psych)
fix(survey)
likert_sat <- survey[1:4]
likert_loyalty <- survey[6]
likert_invest <- survey[7]
my_list <- list(likert_sat, likert_loyalty, likert_invest)
View(my_list)
View(likert_sat)
View(survey)
View(survey)
# Response frequencies
responses_fr <- lapply(my_list, response.frequencies)
responses_fr
likert_sat <- likert_sat %>%
mutate_if(is.numeric, as.factor)
summary(likert_data)
likert_data_results <- likert(likert_sat)
plot(likert_data_results, group.order = c("Sat1", "Sat2", "Sat3", "Sat4"))
# Total number of complete cases
data_complete <- nrow(na.omit(likert_sat))
# Total number of incomplete cases by variable
colSums(is.na(likert_sat))
data_missing <- sum(colSums(is.na(likert_sat))) # Total number
data_summary <- cbind(data_complete, data_missing)
rownames(data_summary) <- paste("Survey reponses",sep = " ")
data_summary
likert_sat <- likert_sat %>%
mutate_if(is.factor, as.numeric)
cronbachs_alpha <- alpha(likert_sat)
cronbachs_alpha_summary <- cronbachs_alpha$item.stats
cronbachs_alpha_r <- round(cronbachs_alpha$total$raw_alpha, 3)
cronbachs_alpha_r # No
# Cronbach's alpha if item drop
cronbachs_alpha_drop <- round(cronbachs_alpha$alpha.drop$raw_alpha, 3)
cronbachs_alpha_drop
cronbachs_alpha_drop_summary <- ifelse(cronbachs_alpha_drop >
cronbachs_alpha_r, "BAD", "OK")
cronbachs_alpha_drop_summary # Note. None of the items yield a higher alpha value if we drop them :D
# Cronbach's alpha for Corrected - Item Total Correlation
cronbachs_alpha_corrected <- round(cronbachs_alpha$item.stats$r.drop, 3)
cronbachs_alpha_corrected
cronbachs_alpha_corrected_summary <- ifelse(cronbachs_alpha_corrected <
0.3, "BAD", "OK")
cronbachs_alpha_corrected_summary # Note. None of items present a a correlation lower than .3 :D
likert_loyalty <- likert_loyalty %>%
mutate_if(is.numeric, as.factor)
summary(likert_loyalty)
likert_loyalty_results <- likert(likert_loyalty)
plot(likert_loyalty_results)
data_complete <- nrow(na.omit(likert_loyalty))
# Missing cases
data_missing <- colSums(is.na(likert_loyalty))
# Data summary
cbind(data_complete, data_missing)
fix(Custom)
names(Custom)
#[1] "Sat1"             "Sat2"             "Sat3"             "Sat4"
#[5] "CustSatMean"          "Custloyalty"      "INvestMore"       "SexOfSalesperson"
#make the dataset live
attach(Custom)
#Figure 7.3
modelf7_3=lm(Custloyalty ~ Sat1 + Sat2 + Sat3 + Sat4 + SexOfSalesperson)
modelf7_3
summary(modelf7_3)
coef(modelf7_3)
library(QuantPsyc)
lm.beta(modelf7_3)
#install.packages("trendyy")
#remotes::install_github("josiahparry/gtrendsR", "interest_refactor")
library(trendyy)
library(dplyr)
library(ggplot2)
# http://josiahparry.com/post/2019-05-25-introducing-trendyy/
# 1. Create Vector for search terms ----
#analytics <- c("paithani saree","banarasi saree","kantha saree","sambalpuri saree","khandua saree")
#analytics <- c("pasapali","bomkai","khandua silk")
#analytics  <- c("virat kohli","steve smith")
analytics <- c("salvos")
# 2. Get Query ----
analytics_trends <- trendy(analytics, from = "2015-01-01", to = Sys.Date())
# 3. Get Summary of trends ----
analytics_trends
# 4. Get Trend Data
get_interest(analytics_trends)
get_interest_country(analytics_trends)
# 5. Plot Trends
analytics_trends %>%
get_interest   %>%
ggplot(aes(date, hits, color = keyword)) +
#  geom_line() +
geom_smooth(span=0.5,se=FALSE)+
geom_point(alpha = .2) +
theme_minimal() +
theme(legend.position = "bottom") +
theme(plot.title = element_text(hjust = 0.5))+
labs(x = "",
y = "Relative Search Popularity",
title = "Google Search Popularity") +
#  ylim(0,10) +
ggsave("myplot.png")
# 6. Get Related Queries
analytics_trends %>%
get_related_queries() %>%
group_by(keyword) %>%
sample_n(20)
#https://www.salvationarmy.org.au/about-us/news-and-stories/media-newsroom/2014-doorknock-result/
e
.010.
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/ebp_exercise")
#https://cran.r-project.org/web/packages/DataExplorer/vignettes/dataexplorer-intro.html
#https://www.mymarketresearchmethods.com/types-of-data-nominal-ordinal-interval-ratio/
#https://stats.idre.ucla.edu/r/dae/ordinal-logistic-regression/
#https://www.r-bloggers.com/how-to-perform-ordinal-logistic-regression-in-r/
# Libraries
library(corrplot)
library(tidyverse)
library(tidyquant)
library(readxl)
library(forcats)
library(stringr)
library(kableExtra)
library(skimr)
library(GGally)
#remove.packages("DataExplorer")
#install.packages("DataExplorer",dependencies = TRUE)
library(caret)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(vcd)
library(UsingR)
library(DataExplorer)
####
# 1. Data Preparation ----
####
perf.data <- read.csv("data.csv",header = TRUE)
glimpse(perf.data)
# Returns variable types of the dataset
str(final.data)
# Provides information on missing values in the dataset
skim(final.data)
# Quickly visualize the data by using the plot_str function
perf.data %>%
plot_str(fontSize=80)
####
# 2. Exploratory Data Analysis ----
####
# Lets get introduced to our dataset
introduce(perf.data)
# Visualize the data
plot_intro(perf.data)
# If fields showed missing data , you can visualize the missing data fields
plot_missing(perf.data)
# Return first 5 rows of the dataset
perf.data %>%
head(5)%>%
kable() %>%
kable_styling(bootstrap_options = c("hover","striped","condensed"),full_width = F)
# Return %ages of workers in different classes of performance group
perf.data %>%
group_by(performance_group) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(pct = n / sum(n))%>%
kable() %>%
kable_styling(bootstrap_options = c("hover","striped","condensed"),full_width = F)
# Drop employee id column as it no impact to our modeling
perf.data <- drop_columns(final.data, c("employee_id"))
# This plot visualisizes frequency distributions for all the discrete features
plot_bar(perf.data)
# A bivirate frequency distribution will show the distribution of discrete features by Performance group
plot_bar(perf.data,with = "customers")
# This plot visualises frequency distribution of all continous variables
# Customers , group size , yrs employed do not have a uniform distribution. Transfers data is mostly 0. Lets convert it to factor
plot_histogram(perf.data)
perf.data <- update_columns(perf.data,"transfers",as.factor)
perf.data %>%
head(5)
# QQ plot is a way to visualise the deviation from a specific probability distribution.
# customers , group_size and yrs_employed are skewed.
qq_data <- perf.data[, c("customers", "group_size", "test_score","yrs_employed")]
plot_qq(qq_data, sampled_rows = 1000L)
# Lets run the qq plot after converting the variables into the log values.
log_qq_data <- update_columns(qq_data, c("customers", "group_size", "yrs_employed"), function(x) log(x + 1))
plot_qq(log_qq_data[, c("customers", "group_size", "yrs_employed")], sampled_rows = 1000L)
# Lets transform the data in the perf.data table with the log values of the variables
perf.data <- update_columns(perf.data, c("customers", "group_size", "yrs_employed"), function(x) log(x + 1))
# Now lets visualize the QQ plot by the feature Performance_group
qq_data <- perf.data[, c("customers", "group_size", "yrs_employed", "test_score","performance_group")]
plot_qq(qq_data, by = "performance_group", sampled_rows = 1000L)
####
# 3. Correlation Analysis ----
####
# Plot correlation plot to display relationships
plot_correlation(na.omit(perf.data), maxcat = 5L)
# You can generate correlation plot for character and discrete variables separately also
plot_correlation(na.omit(perf.data), type = "c")
plot_correlation(na.omit(perf.data), type = "d")
####
# 4. Define Training and Test Dataset ----
####
#Dividing data into training and test set
#Random sampling
samplesize = 0.80*nrow(perf.data)
set.seed(100)
index = sample(seq_len(nrow(perf.data)), size = samplesize)
#Creating training and test set
datatrain = perf.data[index,]
datatest = perf.data[-index,]
####
# 5. Modeling - Ordinal Logistic Regression ----
####
# load car package
library(car)
# Use the Intercept Model as the initial model;
lower.rm <- polr(performance_group ~ 1,data=datatrain)
summary(lower.rm)
# Define upper model;
upper.rm <- polr(performance_group ~ .,data=datatrain)
summary(upper.rm)
####
# 6. Step AIC Model ----
####
# Step AIC model :
stepwise.rm <- stepAIC(upper.rm)
upper.rm <- MASS::polr(performance_group ~ .,data=datatrain)
summary(upper.rm)
datatrain
upper.rm <- polr(performance_group ~ yrs_employed + manager_hire + test_score
+ group_size + concern_flag + mobile_flag + customers
+ high_hours_flag + transfers + city,data=datatrain)
summary(upper.rm)
stepwise.rm <- stepAIC(upper.rm)
library(car)
sort(vif(lower.rm), decreasing = TRUE)
sort(vif(upper.rm),decreasing = TRUE)
sort(vif(stepwise.rm),decreasing = TRUE)
ctable <- coef(summary(stepwise.rm))
odds_ratio <- exp(coef(summary(stepwise.rm))[ , c("Value")])
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
coef_summary <- cbind(ctable, as.data.frame(odds_ratio, nrow = nrow(ctable), ncol = 1), "p value" = p)
kable(coef_summary[1:(nrow(coef_summary) - 2), ]) %>%
kable_styling(bootstrap_options = c("hover","striped","condensed"),full_width = F)
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/ebp_exercise")
#https://cran.r-project.org/web/packages/DataExplorer/vignettes/dataexplorer-intro.html
#https://www.mymarketresearchmethods.com/types-of-data-nominal-ordinal-interval-ratio/
#https://stats.idre.ucla.edu/r/dae/ordinal-logistic-regression/
#https://www.r-bloggers.com/how-to-perform-ordinal-logistic-regression-in-r/
# Libraries
library(corrplot)
library(tidyverse)
library(tidyquant)
library(readxl)
library(forcats)
library(stringr)
library(kableExtra)
library(skimr)
library(GGally)
#remove.packages("DataExplorer")
#install.packages("DataExplorer",dependencies = TRUE)
library(caret)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(vcd)
library(UsingR)
library(DataExplorer)
####
# 1. Data Preparation ----
####
perf.data <- read.csv("data.csv",header = TRUE)
glimpse(perf.data)
# Returns variable types of the dataset
str(final.data)
# Provides information on missing values in the dataset
skim(final.data)
# Quickly visualize the data by using the plot_str function
perf.data %>%
plot_str(fontSize=80)
####
# 2. Exploratory Data Analysis ----
####
# Lets get introduced to our dataset
introduce(perf.data)
# Visualize the data
plot_intro(perf.data)
# If fields showed missing data , you can visualize the missing data fields
plot_missing(perf.data)
# Return first 5 rows of the dataset
perf.data %>%
head(5)%>%
kable() %>%
kable_styling(bootstrap_options = c("hover","striped","condensed"),full_width = F)
# Return %ages of workers in different classes of performance group
perf.data %>%
group_by(performance_group) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(pct = n / sum(n))%>%
kable() %>%
kable_styling(bootstrap_options = c("hover","striped","condensed"),full_width = F)
# Drop employee id column as it no impact to our modeling
perf.data <- drop_columns(final.data, c("employee_id"))
# This plot visualisizes frequency distributions for all the discrete features
plot_bar(perf.data)
# A bivirate frequency distribution will show the distribution of discrete features by Performance group
plot_bar(perf.data,with = "customers")
# This plot visualises frequency distribution of all continous variables
# Customers , group size , yrs employed do not have a uniform distribution. Transfers data is mostly 0. Lets convert it to factor
plot_histogram(perf.data)
perf.data <- update_columns(perf.data,"transfers",as.factor)
perf.data %>%
head(5)
# QQ plot is a way to visualise the deviation from a specific probability distribution.
# customers , group_size and yrs_employed are skewed.
qq_data <- perf.data[, c("customers", "group_size", "test_score","yrs_employed")]
plot_qq(qq_data, sampled_rows = 1000L)
# Lets run the qq plot after converting the variables into the log values.
log_qq_data <- update_columns(qq_data, c("customers", "group_size", "yrs_employed"), function(x) log(x + 1))
plot_qq(log_qq_data[, c("customers", "group_size", "yrs_employed")], sampled_rows = 1000L)
# Lets transform the data in the perf.data table with the log values of the variables
perf.data <- update_columns(perf.data, c("customers", "group_size", "yrs_employed"), function(x) log(x + 1))
# Now lets visualize the QQ plot by the feature Performance_group
qq_data <- perf.data[, c("customers", "group_size", "yrs_employed", "test_score","performance_group")]
plot_qq(qq_data, by = "performance_group", sampled_rows = 1000L)
####
# 3. Correlation Analysis ----
####
# Plot correlation plot to display relationships
plot_correlation(na.omit(perf.data), maxcat = 5L)
# You can generate correlation plot for character and discrete variables separately also
plot_correlation(na.omit(perf.data), type = "c")
plot_correlation(na.omit(perf.data), type = "d")
####
# 4. Define Training and Test Dataset ----
####
#Dividing data into training and test set
#Random sampling
samplesize = 0.80*nrow(perf.data)
set.seed(100)
index = sample(seq_len(nrow(perf.data)), size = samplesize)
#Creating training and test set
datatrain = perf.data[index,]
datatest = perf.data[-index,]
datatest
# load car package
library(car)
# Use the Intercept Model as the initial model;
lower.rm <- polr(performance_group ~ 1,data=datatrain)
summary(lower.rm)
# Define upper model;
upper.rm <- polr(performance_group ~ yrs_employed + manager_hire + test_score
+ group_size + concern_flag + mobile_flag + customers
+ high_hours_flag + transfers + city,data=datatrain)
summary(upper.rm)
exp(upper.rm$coefficients)
setwd("~/Google Drive/Data Analytics/Predictive HR Analytics/ebp_exercise")
#https://cran.r-project.org/web/packages/DataExplorer/vignettes/dataexplorer-intro.html
#https://www.mymarketresearchmethods.com/types-of-data-nominal-ordinal-interval-ratio/
#https://stats.idre.ucla.edu/r/dae/ordinal-logistic-regression/
#https://www.r-bloggers.com/how-to-perform-ordinal-logistic-regression-in-r/
# Libraries
library(corrplot)
library(tidyverse)
library(tidyquant)
library(readxl)
library(forcats)
library(stringr)
library(kableExtra)
library(skimr)
library(GGally)
#remove.packages("DataExplorer")
#install.packages("DataExplorer",dependencies = TRUE)
library(caret)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(vcd)
library(UsingR)
library(DataExplorer)
####
# 1. Data Preparation ----
####
perf.data <- read.csv("data.csv",header = TRUE)
glimpse(perf.data)
# Returns variable types of the dataset
str(final.data)
# Provides information on missing values in the dataset
skim(final.data)
# Quickly visualize the data by using the plot_str function
perf.data %>%
plot_str(fontSize=80)
####
# 2. Exploratory Data Analysis ----
####
# Lets get introduced to our dataset
introduce(perf.data)
# Visualize the data
plot_intro(perf.data)
# If fields showed missing data , you can visualize the missing data fields
plot_missing(perf.data)
# Return first 5 rows of the dataset
perf.data %>%
head(5)%>%
kable() %>%
kable_styling(bootstrap_options = c("hover","striped","condensed"),full_width = F)
# Return %ages of workers in different classes of performance group
perf.data %>%
group_by(performance_group) %>%
summarise(n = n()) %>%
ungroup() %>%
mutate(pct = n / sum(n))%>%
kable() %>%
kable_styling(bootstrap_options = c("hover","striped","condensed"),full_width = F)
# Drop employee id column as it no impact to our modeling
perf.data <- drop_columns(final.data, c("employee_id"))
# This plot visualisizes frequency distributions for all the discrete features
plot_bar(perf.data)
# A bivirate frequency distribution will show the distribution of discrete features by Performance group
plot_bar(perf.data,with = "customers")
# This plot visualises frequency distribution of all continous variables
# Customers , group size , yrs employed do not have a uniform distribution. Transfers data is mostly 0. Lets convert it to factor
plot_histogram(perf.data)
perf.data <- update_columns(perf.data,"transfers",as.factor)
perf.data %>%
head(5)
# QQ plot is a way to visualise the deviation from a specific probability distribution.
# customers , group_size and yrs_employed are skewed.
qq_data <- perf.data[, c("customers", "group_size", "test_score","yrs_employed")]
plot_qq(qq_data, sampled_rows = 1000L)
# Lets run the qq plot after converting the variables into the log values.
log_qq_data <- update_columns(qq_data, c("customers", "group_size", "yrs_employed"), function(x) log(x + 1))
plot_qq(log_qq_data[, c("customers", "group_size", "yrs_employed")], sampled_rows = 1000L)
# Lets transform the data in the perf.data table with the log values of the variables
perf.data <- update_columns(perf.data, c("customers", "group_size", "yrs_employed"), function(x) log(x + 1))
# Now lets visualize the QQ plot by the feature Performance_group
qq_data <- perf.data[, c("customers", "group_size", "yrs_employed", "test_score","performance_group")]
plot_qq(qq_data, by = "performance_group", sampled_rows = 1000L)
####
# 3. Correlation Analysis ----
####
# Plot correlation plot to display relationships
plot_correlation(na.omit(perf.data), maxcat = 5L)
# You can generate correlation plot for character and discrete variables separately also
plot_correlation(na.omit(perf.data), type = "c")
plot_correlation(na.omit(perf.data), type = "d")
####
# 4. Define Training and Test Dataset ----
####
#Dividing data into training and test set
#Random sampling
samplesize = 0.80*nrow(perf.data)
set.seed(100)
index = sample(seq_len(nrow(perf.data)), size = samplesize)
#Creating training and test set
datatrain = perf.data[index,]
datatest = perf.data[-index,]
####
# 5. Modeling - Ordinal Logistic Regression ----
####
# load car package
library(car)
# Use the Intercept Model as the initial model;
lower.rm <- polr(performance_group ~ 1,data=datatrain)
summary(lower.rm)
# Define upper model;
upper.rm <- polr(performance_group ~ yrs_employed + manager_hire + test_score
+ group_size + concern_flag + mobile_flag + customers
+ high_hours_flag + transfers + city,data=datatrain)
summary(upper.rm)
exp(upper.rm$coefficients)
stepwise.rm <- stepAIC(upper.rm)
####
# 8. Feature Importance  ----
####
ctable <- coef(summary(stepwise.rm))
odds_ratio <- exp(coef(summary(stepwise.rm))[ , c("Value")])
p <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
coef_summary <- cbind(ctable, as.data.frame(odds_ratio, nrow = nrow(ctable), ncol = 1), "p value" = p)
kable(coef_summary[1:(nrow(coef_summary) - 2), ]) %>%
kable_styling(bootstrap_options = c("hover","striped","condensed"),full_width = F)
predictperformance = predict(stepwise.rm,datatest[,-1])
table(datatest$performance_group, predictperformance)
mean(as.character(datatest$performance_group) != as.character(predictperformance))
#Plotting the effects
library("effects")
Effect(focal.predictors = "yrs_employed",stepwise.rm)
plot(Effect(focal.predictors = "manager_hireY",stepwise.rm))
plot(Effect(focal.predictors = c("yrs_employed", "manager_hireY"),stepwise.rm))
install.packages("effects")
#Plotting the effects
library("effects")
Effect(focal.predictors = "yrs_employed",stepwise.rm)
plot(Effect(focal.predictors = "manager_hireY",stepwise.rm))
plot(Effect(focal.predictors = c("yrs_employed", "manager_hireY"),stepwise.rm))
Effect(focal.predictors = "yrs_employed",stepwise.rm)
plot(Effect(focal.predictors = "manager_hire",stepwise.rm))
plot(Effect(focal.predictors = c("yrs_employed", "manager_hire"),stepwise.rm))
plot(Effect(focal.predictors = "yrs_employed",stepwise.rm))
plot(Effect(focal.predictors = c("yrs_employed", "high_hours_flag"),stepwise.rm))
plot(Effect(focal.predictors = c("yrs_employed", "transfers"),stepwise.rm))
plot(Effect(focal.predictors = c("yrs_employed", "group_size"),stepwise.rm))
plot(Effect(focal.predictors = "manager_hire",stepwise.rm))
plot(Effect(focal.predictors = c("group_size", "manager_hire"),stepwise.rm))
